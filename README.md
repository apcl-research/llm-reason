# Do Code LLMs Do Static Analysis?

Proposed by:
- [Chia-Yi Su](https://chiayisu.github.io/)
- [Collin McMillan](https://sdf.org/~cmc/)

## Quick link
- [To-do list](#to-do-list)
- [Note](#note)
- [AST generation](#ast-generation)
- [Callgraph generation](#callgraph-generation)
- [Codellama](#Codellama)


## To-do list

To set up your local environment, run the following command. We recommend the use of a virtual environment for running the experiments.
```
pip install -r requirements.txt
```
For finetuning or running codellama related experiments, use:
```
pip install -r requirements_codellama.txt
```

# Note 
you need to have the key from OpenAI to run the GPT model and the key from Google to run the Gemini model. 

You also need to download the CodeLlama 13B model in Meta [Huggingace](https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf). You need to obtain permission for using the model as well.

For GPT and Gemini, ``*_example.py`` means with in-context learning where * is either GPT or Gemini e.g. ``gpt_example.py`` means gpt models with in-context learning.


## AST generation

### Generation
- For AST generation experiements, visit related directory for desired models. For example, if you want to run the ``codellama`` model, you would visit ``srcml/java/codellama``.
- Each directory has its related ``yaml`` file for configuration.
- The paramters for generating srcml are as follows:

```
function_file_dir: file location of the functions
q90_fid_file: file location of the function id for testset
OUT_FILENAME: filename for the output file
model_id: file location of your codellama model
```
### Metrics
Run the following command in ``srcml/java/`` directory:
```
python3 srcml_metrics.py
```

The parameters are as follows:
```
srcml_tools_filename: filename for srcml generated by the tool
srcml_gpt_filename: filename for srcml generated by LLMs
```

## Callgraph generation

### Generation
- For callgraph generation experiements, visit related directory for desired models. For example, if you want to run the ``codellama`` model, you would visit ``callgraph/java/codellama``.
- Each directory has its related ``yaml`` file for configuration.
- The paramters for generating srcml are as follows:

```
model_id: file location of your codellama model
OUT_FILENAME: filename for the output file
callgraph_data_file: file location of function and call methods
```
### Metrics
For models other than codellama, run the following command in ``callgraph/java/`` directory:
```
python3 callgraph_metric.py
```
The parameters are as follows:
```
dat: filename for callgraph generated by LLMs
```

For codellam, run the follwing command in ``callgraph/java/codellama/`` directory:
```
python3 callgraph_metric.py
```

The parameter includes:
```
llm_out_file: filename of callgraph generated by codellam for running metrics script in codellama diretectory
```


## Dataflow graph generation

### Generation
- For dataflow graph generation experiements, visit related directory for desired models. For example, if you want to run the ``codellama`` model, you would visit ``dataflow/java/codellama``.
- Each directory has its related ``yaml`` file for configuration.
- The paramters for generating srcml are as follows:

```
model_id: file location of your codellama model
OUT_FILENAME: filename for the output file
datafile: file location of function and call methods
```
### Metrics
For models other than codellama, run the following command in ``dataflow/java/`` directory:
```
python3 metric.py
```
The parameters are as follows:
```
tools_filename: filename for dataflow graph generated by tools
llm_filename: filename for dataflow graph generated by llms
```

For codellam, run the follwing command in ``dataflow/java/codellama/`` directory:
```
python3 metrics.py
```

The parameter includes:
```
tools_filename: filename for dataflow graph generated by tools
llm_filename: filename for dataflow graph generated by llms
```

## Codellama

### Compile data
To compile data for train/val/test codellama, visit related directory for the desired task. For example, if you want to compile data for srcml, you would visit ``srcml_data`` directory.

The command is as follows:

```
python3 srcml_data_java.py
```

### Finetune
To finetune codellama, visit related directory for the desired task. For example, if you want to finetune codellama for srcml, you would visit ``srcml_finetune`` directory.

The command is as follows:

```
CUDA_DEVICE_ORDER='PCI_BUS_ID' CUDA_VISIBLE_DEVICES='2,3' python3 qlora_with_val_srcml_java.py --model_name_or_path CodeLlama-13b-Instruct-hf/ --ddp_find_unused_parameters False --bf16 --dataset data/srcml_java_train.json --max_steps 240 --output_dir srcml_java_pretrained --per_device_train_batch_size 4
```

### Generate 
To generate the results after finetunining codellama, visit related directory for the desired task. For example, if you want to generate srcml after finetuning codellama, you would visit ``srcml_generate`` directory.

The command is as follows:

```
CUDA_DEVICE_ORDER='PCI_BUS_ID' CUDA_VISIBLE_DEVICES='2,3' python3 generate_srcml_java.py
```


